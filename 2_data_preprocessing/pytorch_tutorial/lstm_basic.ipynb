{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```nn.LSTM``` is a class within the PyTorch framework, specially part fo the ```torch.nn``` module. It is used to create an instance of the ```Long short-term memory(LSTM)``` layer. \n",
    "\n",
    "### Key Parameters of ```nn.LSTM```\n",
    "\n",
    "* ```input_size:``` The number of expected features in the input ```x```. \n",
    "* ```hidden_size:``` The number of features in the hidden state ```h```\n",
    "* ```num_layers(optional):``` Number of recurrent layer. E.g., setting ```num_layers=2``` would mean stacking two LSTMs together to form a stacked LSTM, wiht the second LSTM taking in outputs of the first LSTM and computing the final result. \n",
    "* ```bias(optional):``` If ```False```, then the layer does not use bias weights ```b_ih``` and ```b_hh```. Default is ```True```. \n",
    "* ```batch_first(optional):``` If ```True```, then the input and output tensors are provided as (batch, seq, feature). Default is ```False```, which expects (seq, batch,feature).\n",
    "* ```dropout(optional):``` If ```non-zero```, indroduce a Dropout layer on the output of each LSTM layer expect the last layer, with dropout probability equal to dropout. Default is ```0```.\n",
    "* ```bidirectional(optional):``` If ```True```, becomes a bidirectional LSTM. Default is ```False```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embadding_dim, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embadding = nn.Embedding(vocab_size, embadding_dim) # embadding layer\n",
    "        self.lstm = nn.LSTM(embadding_dim, hidden_size, batch_first=True) # LSTM Layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size) # fully connected layer to produce the output\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # enbadded input words\n",
    "        x = self.embadding(x)\n",
    "\n",
    "        # initiate hidden state and cell state which zero\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        # forward propagate the LSTM\n",
    "        out,(hn,cn) = self.lstm(x,(h0, c0))\n",
    "\n",
    "        # pss the output of the last time step to the fully connected layer\n",
    "        out = self.fc(out[:,-1,:])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "vocab_size = 10 # Size of the vocabulary\n",
    "embadding_dim = 4 # dimension of the embadding vactors\n",
    "hidden_size=10 # number of features in the hidden state\n",
    "output_size = 1 # number of output classes per timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1908648996.py, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 17\u001b[0;36m\u001b[0m\n\u001b[0;31m    loss =\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Create model \n",
    "model = LSTMModel(vocab_size, embadding_dim, hidden_size, output_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Sample data (batch size, sequence length)\n",
    "input = torch.tensor([[1,2,3],[2,3,4]])\n",
    "target = torch.tensor([[4.0],[5.0]])\n",
    "\n",
    "# training loop \n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input)\n",
    "    loss = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 26.4189\n",
      "Epoch [20/100], Loss: 18.6157\n",
      "Epoch [30/100], Loss: 8.9903\n",
      "Epoch [40/100], Loss: 2.8900\n",
      "Epoch [50/100], Loss: 0.6973\n",
      "Epoch [60/100], Loss: 0.3916\n",
      "Epoch [70/100], Loss: 0.4069\n",
      "Epoch [80/100], Loss: 0.3324\n",
      "Epoch [90/100], Loss: 0.2670\n",
      "Epoch [100/100], Loss: 0.2398\n"
     ]
    }
   ],
   "source": [
    "# Where we want to get the output at every time step \n",
    "class LSTM2(nn.Module):\n",
    "    def __init__(self, vocab_size, embadding_dim, hidden_size,output_size):\n",
    "        super(LSTM2, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embdadding = nn.Embedding(vocab_size, embadding_dim)\n",
    "        self.rnn = nn.LSTM(embadding_dim, hidden_size, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embdadding(x)\n",
    "        h0 = torch.zeros(1, x.size(0),self.hidden_size)\n",
    "        c0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        out,(hn,cn) = self.rnn(x,(h0,c0))\n",
    "        out =self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Paramenters \n",
    "vocab_size = 10 # size of the vocabulary \n",
    "embadding_dim = 4 # dimension of the embdading vactors \n",
    "hidden_size = 10 # number of features in the hidden state \n",
    "output_size = 1 # number of output classses per timesteps \n",
    "\n",
    "# Create the model \n",
    "model = LSTM2(vocab_size, embadding_dim, hidden_size, output_size)\n",
    "\n",
    "# loss and optimizer \n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr= 0.01)\n",
    "\n",
    "\n",
    "# Sample data(batch size, sequence length)\n",
    "input = torch.tensor([[1,2,3],[2,3,4]])\n",
    "target = torch.tensor([[[4.0],[5.0],[6.0]],[[5.0],[6.0],[7.0]]])\n",
    "\n",
    "# Training loop \n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input)\n",
    "    loss = criterion(outputs, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values: [[[1.6227318]\n",
      "  [4.0620785]\n",
      "  [5.894954 ]]]\n"
     ]
    }
   ],
   "source": [
    "# Test the model \n",
    "model.eval()\n",
    "test_input = torch.tensor([[3,4,5]])\n",
    "predicted = model(test_input)\n",
    "print(f\"Predicted values: {predicted.detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```nn.Embadding``` layer maps each interger int he input sequence to a high-dimentional vector. This layer is particularly useful when dealing with words where each word is reporesented as unique integer.\n",
    "```nn.Embadding``` layer transforms each integer in the input tensor into a embadding vactor. The output shape form the embading layer becomes ```(batch_size,sequnce_length, embadding_size)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
