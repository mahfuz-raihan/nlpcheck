{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```nn.LSTM``` is a class within the PyTorch framework, specially part fo the ```torch.nn``` module. It is used to create an instance of the ```Long short-term memory(LSTM)``` layer. \n",
    "\n",
    "### Key Parameters of ```nn.LSTM```\n",
    "\n",
    "* ```input_size:``` The number of expected features in the input ```x```. \n",
    "* ```hidden_size:``` The number of features in the hidden state ```h```\n",
    "* ```num_layers(optional):``` Number of recurrent layer. E.g., setting ```num_layers=2``` would mean stacking two LSTMs together to form a stacked LSTM, wiht the second LSTM taking in outputs of the first LSTM and computing the final result. \n",
    "* ```bias(optional):``` If ```False```, then the layer does not use bias weights ```b_ih``` and ```b_hh```. Default is ```True```. \n",
    "* ```batch_first(optional):``` If ```True```, then the input and output tensors are provided as (batch, seq, feature). Default is ```False```, which expects (seq, batch,feature).\n",
    "* ```dropout(optional):``` If ```non-zero```, indroduce a Dropout layer on the output of each LSTM layer expect the last layer, with dropout probability equal to dropout. Default is ```0```.\n",
    "* ```bidirectional(optional):``` If ```True```, becomes a bidirectional LSTM. Default is ```False```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embadding_dim, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embadding = nn.Embedding(vocab_size, embadding_dim) # embadding layer\n",
    "        self.lstm = nn.LSTM(embadding_dim, hidden_size, batch_first=True) # LSTM Layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size) # fully connected layer to produce the output\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # enbadded input words\n",
    "        x = self.embadding(x)\n",
    "\n",
    "        # initiate hidden state and cell state which zero\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        # forward propagate the LSTM\n",
    "        out,(hn,cn) = self.lstm(x,(h0, c0))\n",
    "\n",
    "        # pss the output of the last time step to the fully connected layer\n",
    "        out = self.fc(out[:,-1,:])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "vocab_size = 10 # Size of the vocabulary\n",
    "embadding_dim = 4 # dimension of the embadding vactors\n",
    "hidden_size=10 # number of features in the hidden state\n",
    "output_size = 1 # number of output classes per timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 17.2338\n",
      "Epoch [20/100], Loss: 11.9545\n",
      "Epoch [30/100], Loss: 4.8258\n",
      "Epoch [40/100], Loss: 0.3735\n",
      "Epoch [50/100], Loss: 0.2054\n",
      "Epoch [60/100], Loss: 0.1379\n",
      "Epoch [70/100], Loss: 0.0023\n",
      "Epoch [80/100], Loss: 0.0187\n",
      "Epoch [90/100], Loss: 0.0007\n",
      "Epoch [100/100], Loss: 0.0016\n"
     ]
    }
   ],
   "source": [
    "# Create model \n",
    "model = LSTMModel(vocab_size, embadding_dim, hidden_size, output_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Sample data (batch size, sequence length)\n",
    "input = torch.tensor([[1,2,3],[2,3,4]])\n",
    "target = torch.tensor([[4.0],[5.0]])\n",
    "\n",
    "# Training loop \n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input)\n",
    "    loss = criterion(outputs, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 26.4189\n",
      "Epoch [20/100], Loss: 18.6157\n",
      "Epoch [30/100], Loss: 8.9903\n",
      "Epoch [40/100], Loss: 2.8900\n",
      "Epoch [50/100], Loss: 0.6973\n",
      "Epoch [60/100], Loss: 0.3916\n",
      "Epoch [70/100], Loss: 0.4069\n",
      "Epoch [80/100], Loss: 0.3324\n",
      "Epoch [90/100], Loss: 0.2670\n",
      "Epoch [100/100], Loss: 0.2398\n"
     ]
    }
   ],
   "source": [
    "# Where we want to get the output at every time step \n",
    "class LSTM2(nn.Module):\n",
    "    def __init__(self, vocab_size, embadding_dim, hidden_size,output_size):\n",
    "        super(LSTM2, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embdadding = nn.Embedding(vocab_size, embadding_dim)\n",
    "        self.rnn = nn.LSTM(embadding_dim, hidden_size, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embdadding(x)\n",
    "        h0 = torch.zeros(1, x.size(0),self.hidden_size)\n",
    "        c0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        out,(hn,cn) = self.rnn(x,(h0,c0))\n",
    "        out =self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Paramenters \n",
    "vocab_size = 10 # size of the vocabulary \n",
    "embadding_dim = 4 # dimension of the embdading vactors \n",
    "hidden_size = 10 # number of features in the hidden state \n",
    "output_size = 1 # number of output classses per timesteps \n",
    "\n",
    "# Create the model \n",
    "model = LSTM2(vocab_size, embadding_dim, hidden_size, output_size)\n",
    "\n",
    "# loss and optimizer \n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr= 0.01)\n",
    "\n",
    "\n",
    "# Sample data(batch size, sequence length)\n",
    "input = torch.tensor([[1,2,3],[2,3,4]])\n",
    "target = torch.tensor([[[4.0],[5.0],[6.0]],[[5.0],[6.0],[7.0]]])\n",
    "\n",
    "# Training loop \n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input)\n",
    "    loss = criterion(outputs, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values: [[[1.6227318]\n",
      "  [4.0620785]\n",
      "  [5.894954 ]]]\n"
     ]
    }
   ],
   "source": [
    "# Test the model \n",
    "model.eval()\n",
    "test_input = torch.tensor([[3,4,5]])\n",
    "predicted = model(test_input)\n",
    "print(f\"Predicted values: {predicted.detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```nn.Embadding``` layer maps each interger int he input sequence to a high-dimentional vector. This layer is particularly useful when dealing with words where each word is reporesented as unique integer.\n",
    "\n",
    "```nn.Embadding``` layer transforms each integer in the input tensor into a embadding vactor. The output shape form the embading layer becomes ```(batch_size,sequnce_length, embadding_size)```\n",
    "\n",
    "If ```embadding_dim`` is 4, as in the example, the shape after the embadding layer will be (2,3,4).\n",
    "\n",
    "```LSTM Layer``` When this tensor is passed through the ```nn.LSTM``` layer, the LSTM processes each dsequence of embadded vactors. The ```nn.LSTM``` layer outputs three tensors: ```the output tensor```, ```the hidden state``` and ```the cell state```. The ```output tensor``` from the LSTM hase the general shape(```batch_size, sequence_length, num_directions * hidden_size```), both of these ```state tensors the cell and the hidden``` hase the shape(```num_layers * num_direction, batch_size, hidden_size```).\n",
    "\n",
    "```Fully Connected Layer``` the model can uses the output at the last step or at every timestep of the sequence to make a prediction. \n",
    "\n",
    "I we want to prediction at the last time steps, output at is sliced form the LSTM output tensor wiht ```out[:,-1,:]```, which reduce its shape to (```batch_size, hidden_size```), or (2,10). The sliced ouptut is then passed through a fully connnected layers (```nn.Liner```), which is designed to map the LSTM's hidden state to the desired output sizer. \n",
    "\n",
    "If we want the prediciton at every timesteps, output no need to slice the RNN output tensor simply pass it to (```nn.Liner```), which is designed to map the ```RNN```'s hidden state to the desired output size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
