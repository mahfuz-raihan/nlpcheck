{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```nn.LSTM``` is a class within the PyTorch framework, specially part fo the ```torch.nn``` module. It is used to create an instance of the ```Long short-term memory(LSTM)``` layer. \n",
    "\n",
    "### Key Parameters of ```nn.LSTM```\n",
    "\n",
    "* ```input_size:``` The number of expected features in the input ```x```. \n",
    "* ```hidden_size:``` The number of features in the hidden state ```h```\n",
    "* ```num_layers(optional):``` Number of recurrent layer. E.g., setting ```num_layers=2``` would mean stacking two LSTMs together to form a stacked LSTM, wiht the second LSTM taking in outputs of the first LSTM and computing the final result. \n",
    "* ```bias(optional):``` If ```False```, then the layer does not use bias weights ```b_ih``` and ```b_hh```. Default is ```True```. \n",
    "* ```batch_first(optional):``` If ```True```, then the input and output tensors are provided as (batch, seq, feature). Default is ```False```, which expects (seq, batch,feature).\n",
    "* ```dropout(optional):``` If ```non-zero```, indroduce a Dropout layer on the output of each LSTM layer expect the last layer, with dropout probability equal to dropout. Default is ```0```.\n",
    "* ```bidirectional(optional):``` If ```True```, becomes a bidirectional LSTM. Default is ```False```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embadding_dim, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embadding = nn.Embedding(vocab_size, embadding_dim) # embadding layer\n",
    "        self.lstm = nn.LSTM(embadding_dim, hidden_size, batch_first=True) # LSTM Layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size) # fully connected layer to produce the output\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # enbadded input words\n",
    "        x = self.embadding(x)\n",
    "\n",
    "        # initiate hidden state and cell state which zero\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        # forward propagate the LSTM\n",
    "        out,(hn,cn) = self.lstm(x,(h0, c0))\n",
    "\n",
    "        # pss the output of the last time step to the fully connected layer\n",
    "        out = self.fc(out[:,-1,:])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "vocab_size = 10 # Size of the vocabulary\n",
    "embadding_dim = 4 # dimension of the embadding vactors\n",
    "hidden_size=10 # number of features in the hidden state\n",
    "output_size = 1 # number of output classes per timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model \n",
    "model = LSTMModel(vocab_size, embadding_dim, hidden_size, output_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Sample data (batch size, sequence length)\n",
    "input = torch.tensor([[1,2,3],[2,3,4]])\n",
    "target = torch.tensor([[4.0],[5.0]])\n",
    "\n",
    "# training loop \n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input)\n",
    "    loss = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
