{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```nn.RNN``` is a class within the pytorch framework, specifically part of the torch.nn module. It is usef to create a instance of a ```recurrent neural network``` layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Parameter\n",
    "\n",
    "1. ```input_size```: The number of expected features in the input x.\n",
    "2. ```hidden_size```: The number of features in the hidden state h.\n",
    "3. ```num_layers (optional)```: Number of recurrent layers. E.g., setting ```num_layers=2``` would mean stacking two RNNs together to form a stacked RNN, with the second RNN taking in output of the first RNN and computing the final results. \n",
    "4. ```non-linearity(optional)```: the non-linearity to use. Can be either ```tanh``` or ```ReLU``` . Default is ```tanH```.\n",
    "5. ```bias (optional)```: If ```False```, the the layer dones not use bias weight ```b_ih``` and ```b_hh```. Default is ```True```.\n",
    "6. ```batch_size (optional)```: If ```True```, then the input and output tensors are provided as (batch, seq, feature). Default is ```False```, which expects ```(seq,batch,feature)```.\n",
    "7. ```dropout (optional)```: If ```non-zero```, introduces as ```Dropout``` layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default is ```0```. \n",
    "8. ```bidirectional (optional)```: If ```True```, becoumes a bidirectional RNN. Default is ```False```. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Define the RNN model with Embadding \n",
    "\n",
    "class RNN1(nn.Module):\n",
    "    def __init__(self, vocab_size, embadding_dim, hidden_size, output_size):\n",
    "        super(RNN1, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # Define Embadding Layer\n",
    "        self.embadding = nn.Embedding(vocab_size, embadding_dim)\n",
    "        # Define RNN layer\n",
    "        self.rnn = nn.RNN(embadding_dim, hidden_size, batch_first=True)\n",
    "        # Fully connnected layer to produce the output\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # embadded input words\n",
    "        x = self.embadding(x)\n",
    "        # initialize hidden state with zero\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        # Forward propagate the RNN\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        # pass the output of the last time step to the fully connected layer \n",
    "        out = self.fc(out[:,-1,:])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 7.1178\n",
      "Epoch [20/100], Loss: 1.4670\n",
      "Epoch [30/100], Loss: 0.2158\n",
      "Epoch [40/100], Loss: 0.1552\n",
      "Epoch [50/100], Loss: 0.0426\n",
      "Epoch [60/100], Loss: 0.0075\n",
      "Epoch [70/100], Loss: 0.0005\n",
      "Epoch [80/100], Loss: 0.0015\n",
      "Epoch [90/100], Loss: 0.0006\n",
      "Epoch [100/100], Loss: 0.0000\n",
      "Predicted value: [[3.06392]]\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "vocab_size = 10 # Size of the vocabulary (max integer index + 1)\n",
    "embadding_dim = 4 # Dimension of the embadding vectors \n",
    "hidden_size = 10 # number of features in the hidden state\n",
    "output_size = 1 # number of the output classes\n",
    "\n",
    "\n",
    "# Create model \n",
    "model = RNN1(vocab_size=vocab_size, embadding_dim=embadding_dim, hidden_size=hidden_size, output_size=output_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Sample data (batch_sizer, sequence length)\n",
    "input = torch.tensor([[1,2,3],[2,3,4]])\n",
    "targets = torch.tensor([[4.0],[5.0]])\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Test model\n",
    "model.eval()\n",
    "test_input = torch.tensor([[3,4,5]])\n",
    "predicted = model(test_input)\n",
    "print(f'Predicted value: {predicted.detach().numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Where wer want to get the output at every time step: \u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mRNN2\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vocab_size, embadding_dim, hidden_size, output_size):\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28msuper\u001b[39m(RNN2, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# Where wer want to get the output at every time step: \n",
    "\n",
    "class RNN2(nn.Module):\n",
    "    def __init__(self, vocab_size, embadding_dim, hidden_size, output_size):\n",
    "        super(RNN2, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # Define Embadding Layer\n",
    "        self.embadding = nn.Embedding(vocab_size, embadding_dim)\n",
    "        # Define RNN layer\n",
    "        self.rnn = nn.RNN(embadding_dim, hidden_size, batch_first=True)\n",
    "        # Fully connnected layer to produce the output\n",
    "        self.fc = nn.Linear(2*hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # embadded input words\n",
    "        x = self.embadding(x)\n",
    "        # initialize hidden state with zero\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        # Forward propagate the RNN\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        # Apply the fully connected layer to all time step \n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 26.7386\n",
      "Epoch [20/100], Loss: 14.8529\n",
      "Epoch [30/100], Loss: 6.6197\n",
      "Epoch [40/100], Loss: 2.2485\n",
      "Epoch [50/100], Loss: 0.6958\n",
      "Epoch [60/100], Loss: 0.4369\n",
      "Epoch [70/100], Loss: 0.4081\n",
      "Epoch [80/100], Loss: 0.3493\n",
      "Epoch [90/100], Loss: 0.2963\n",
      "Epoch [100/100], Loss: 0.2072\n",
      "Predicted value: [[[2.4346004]\n",
      "  [5.509093 ]\n",
      "  [5.861105 ]]]\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "vocab_size = 10 # Size of the vocabulary (max integer index + 1)\n",
    "embadding_dim = 4 # Dimension of the embadding vectors \n",
    "hidden_size = 10 # number of features in the hidden state\n",
    "output_size = 1 # number of the output classes\n",
    "\n",
    "\n",
    "# Create model \n",
    "model = RNN2(vocab_size=vocab_size, embadding_dim=embadding_dim, hidden_size=hidden_size, output_size=output_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Sample data (batch_sizer, sequence length)\n",
    "input = torch.tensor([[1,2,3],[2,3,4]])\n",
    "targets = torch.tensor([[[4.0],[5.0],[6.0]],[[5.0],[6.0],[7.0]]])\n",
    "\n",
    " \n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Test model\n",
    "model.eval()\n",
    "test_input = torch.tensor([[3,4,5]])\n",
    "predicted = model(test_input)\n",
    "print(f'Predicted value: {predicted.detach().numpy()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
